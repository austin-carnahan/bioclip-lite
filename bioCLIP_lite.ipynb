{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Brd5pLArwUzd"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from datasets import load_dataset\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "from bioclip import TreeOfLifeClassifier\n",
        "\n",
        "\n",
        "# Load the Hugging Face dataset in streaming mode\n",
        "hf_dataset = load_dataset(\"imageomics/TreeOfLife-10M\", split=\"train\", streaming=True)\n",
        "\n",
        "# Get the catalog.csv file URL directly from the dataset info\n",
        "catalog_url = hf_dataset.info.splits[\"train\"].data_files[\"catalog.csv\"]\n",
        "\n",
        "# Load catalog.csv into a pandas DataFrame\n",
        "metadata = pd.read_csv(catalog_url)\n",
        "\n",
        "# Filter the train split from the metadata\n",
        "train_metadata = metadata[metadata['split'] == 'train']\n",
        "\n",
        "# Create custom train, validation, and test splits\n",
        "train_subset, test_subset = train_test_split(train_metadata, test_size=0.1, random_state=42)\n",
        "train_subset, val_subset = train_test_split(train_subset, test_size=0.1, random_state=42)\n",
        "\n",
        "# Save splits for reproducibility\n",
        "train_subset.to_csv(\"train_subset.csv\", index=False)\n",
        "val_subset.to_csv(\"val_subset.csv\", index=False)\n",
        "test_subset.to_csv(\"test_subset.csv\", index=False)\n",
        "\n",
        "print(f\"Training subset: {len(train_subset)} samples\")\n",
        "print(f\"Validation subset: {len(val_subset)} samples\")\n",
        "print(f\"Testing subset: {len(test_subset)} samples\")\n",
        "\n",
        "classifier = TreeOfLifeClassifier()\n",
        "\n",
        "\n",
        "augmentation_pipeline = transforms.Compose([\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.RandomRotation(degrees=15),\n",
        "    transforms.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.1),\n",
        "    transforms.Resize((224, 224)),  # Resize to match model input size\n",
        "    transforms.ToTensor(),  # Convert image to tensor\n",
        "])\n",
        "\n",
        "\n",
        "def filter_dataset(dataset, subset_metadata):\n",
        "    \"\"\"\n",
        "    Filters the Hugging Face dataset using metadata identifiers.\n",
        "    \"\"\"\n",
        "    subset_ids = set(subset_metadata['image_id'])  # Assuming 'image_id' is a unique key in metadata\n",
        "    for sample in dataset:\n",
        "        if sample['image_id'] in subset_ids:  # Match image IDs\n",
        "            yield sample\n",
        "\n",
        "\n",
        "def process_and_generate_soft_targets(dataset_split, batch_size=32):\n",
        "    \"\"\"\n",
        "    Processes the dataset in batches, applies augmentations, and generates predictions using the BioCLIP classifier.\n",
        "    \"\"\"\n",
        "    batch = []\n",
        "    for sample in dataset_split:\n",
        "        # Load the image\n",
        "        image = Image.open(sample[\"image\"])  # Load image from URL or local path\n",
        "\n",
        "        # Apply augmentations\n",
        "        augmented_image = augmentation_pipeline(image)\n",
        "\n",
        "        # Add the augmented image to the batch\n",
        "        batch.append(augmented_image.numpy())  # Convert back to numpy array if needed for BioCLIP\n",
        "\n",
        "        if len(batch) == batch_size:\n",
        "            # Pass the batch to the classifier for prediction\n",
        "            batch_predictions = classifier.predict(batch)\n",
        "            yield batch, batch_predictions\n",
        "            batch = []  # Reset the batch\n",
        "\n",
        "    # Handle remaining samples in the last batch\n",
        "    if batch:\n",
        "        batch_predictions = classifier.predict(batch)\n",
        "        yield batch, batch_predictions\n",
        "\n",
        "\n",
        "# Filter the Hugging Face dataset for each subset\n",
        "train_dataset = filter_dataset(hf_dataset, train_subset)\n",
        "val_dataset = filter_dataset(hf_dataset, val_subset)\n",
        "test_dataset = filter_dataset(hf_dataset, test_subset)\n",
        "\n",
        "# Process the training subset with augmentation\n",
        "print(\"Processing Training Data with Augmentations:\")\n",
        "for i, (batch_images, batch_predictions) in enumerate(process_and_generate_soft_targets(train_dataset, batch_size=32)):\n",
        "    print(f\"Processed Training Batch {i + 1}\")\n",
        "\n",
        "# Process the validation subset without augmentation\n",
        "print(\"Processing Validation Data:\")\n",
        "for i, (batch_images, batch_predictions) in enumerate(process_and_generate_soft_targets(val_dataset, batch_size=32)):\n",
        "    print(f\"Processed Validation Batch {i + 1}\")\n",
        "\n",
        "# Process the test subset without augmentation\n",
        "print(\"Processing Testing Data:\")\n",
        "for i, (batch_images, batch_predictions) in enumerate(process_and_generate_soft_targets(test_dataset, batch_size=32)):\n",
        "    print(f\"Processed Testing Batch {i + 1}\")\n"
      ]
    }
  ]
}